{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "### 1. [Polynomial Regression](#polynomial)\n",
    "### 2. [Step Functions](#stepfunctions) \n",
    "### 3. [Regression Splines](#spline)\n",
    "#### 3.1 [Choosing the Number and Locations of the Knots](#numberknot)\n",
    "### 4. [Smoothing Splines ](#smooth) \n",
    "### 5. [Local Regression](#localregression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression <a class =\"anchor\" id=\"polynomial\"></a>\n",
    "\n",
    "The standard linear model:  \n",
    "$$y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i$$\n",
    "A polynomial function\n",
    "$$y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + ...+\\beta_dx_i^d + \\epsilon_i $$\n",
    "where $\\epsilon_i$ is the error term.  \n",
    "  \n",
    "For large enough degree *d*, a polynomial regression allows us to produce an extremely non-linear curve. Notice that the coefficients in polynomial function can be easily estimated using least squares linear regression because this is just a standard linear model with predictors $x_i, x_i^2, ..., x_i^d$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have computed the fit at a particular value\n",
    "$$ \\hat{f}(x_0)= \\hat\\beta_0 + \\hat\\beta_1x_0 + \\hat\\beta_2x_0^2+ \\hat\\beta_3x_0^3+\\hat\\beta_4x_0^4$$\n",
    "   \n",
    "What is the variance of the fit $Var\\hat{f}(x_0)$?   \n",
    "Least squares returns variance estimates for each of the fitted coefficients $\\hat\\beta_j$, as well as the covariances between pairs of coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Functions <a name=\"stepfunctions\"></a>\n",
    "We break the range of X into bins, and fit a different constant in each bin. This amounts to converting a continuous variable into an *ordered categorical variable*.  \n",
    "  \n",
    "We create cutpoints $c_1, c_2,...,c_K$ in the range of $X$, and then construct $K+1$ new variables: \n",
    "$$C_0(X) = I(X<c_1),$$\n",
    "$$C_1(X) = I(c_1 <= X<c_2),$$\n",
    "$$C_2(X) = I(c_2 <= X<c_3),$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$C_{K-1}(X) = I(c_{K-1} <= X<c_K),$$\n",
    "$$C_K(X) = I(c_K <= X),$$\n",
    "where $I(.)$ is an *indicator function* that returns a 1 if the condition is true, and returns a 0 otherwise. These are sometimes called *dummy variables*.   \n",
    "Notice that for any value of $X, C_0(X)+C_1(X)+...+C_K(X) =1$ since $X$ must be in exactly one of the $K+1$ intervals   \n",
    "We then use least squares to fit a linear model using $C_1(X), C_2(X),...C_K(X)$ as predictors:  \n",
    "$$y_i = \\beta_0 + \\beta_1C_1(x_i)+\\beta_2C_2(x_i)+...+\\beta_KC_K(x_i)+\\epsilon_i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis Functions\n",
    "We fit the model:\n",
    "$$y_i = \\beta_0 + \\beta_1b_1(x_i)+\\beta_2b_2(x_i)+...+\\beta_Kb_K(x_i)+\\epsilon_i$$\n",
    "   \n",
    "Note that the basis functions $b_1(.), b_2(.)...,b_K(.)$ are fixed and known. For polynomial regression, the basis functions are $b_j(x_i) =x_i^j$, and for step functions they are $b_j(x_i) = I(c_j \\leq x_i < c_{j+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Splines <a name=\"spline\"></a>\n",
    "## Piecewise Polynomials  \n",
    "Piecewise polynomial regression involves fitting seperate low-degree polynomials over different region of $X$.   \n",
    "   \n",
    "For example, a *piecewise cubic polynomial* works by fitting a cubic regression model with a single knot at a point $c$ takes the form  \n",
    "  \n",
    "$$\n",
    " y_i = \n",
    "  \\begin{cases} \n",
    "   \\beta_{01} + \\beta_{11}x_i+\\beta_{21}x_i^2 +\\beta_{31}x_i^3+ \\epsilon_i & \\text{if } x \\geq 0 \\\\\n",
    "   \\beta_{02} + \\beta_{12}x_i+\\beta_{22}x_i^2 +\\beta_{32}x_i^3+ \\epsilon_i       & \\text{if } x < 0\n",
    "  \\end{cases}\n",
    "$$\n",
    "   \n",
    "It can be written this way:  \n",
    "$$ y_i = \\displaystyle\\sum_{j=0}^3 \\beta_{j1}(x_i-0)_+^j-\\displaystyle\\sum_{j=0}^3 \\beta_{j2}(0-x_i)_{+}^j +\\epsilon_i$$\n",
    "   \n",
    "where $(a)_+ = a $ if $a \\geq 0$ and 0 otherwise \n",
    "  \n",
    "\n",
    "In other words, we fit two different polynomial functions to the data, one on the subset of the observations with $x_i < c$, and one on the subset of the observations with $x_i \\geq c$  \n",
    "  \n",
    "Using more knots leads to a more flexible piecewise polynomial. In general, if we place $K$ different knots throughout the range of $X$, the we will end up fitting $K+1$ different cubic polynomials.  \n",
    "  \n",
    "But the problem is that we will see a the function is discontinuous. Since each polynomial has four parameters, we are using a total of eight *degrees of freedom* in fitting this piecewise polynomial model.  \n",
    "  \n",
    "To remedy this problem, we can fit a piecewise polynomial under the constraint that the fitted curve must be continuous. We can add two additional constraints: both the first and second derivatives of the piecewise polynomials are continuous (we will take derivatives of 2 polynomial functions above and make them equal).   \n",
    "  \n",
    "So we have 3 constraint equations which effectively frees up 3 degree of freedom. We are left with five degrees of freedom.   \n",
    "In general, a cubic spline with $K$ knots uses a total of $K+4$ degrees of freedom.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Spline Basis Representation \n",
    "  \n",
    "A cubic spline with K knots can be modeled as \n",
    "$$y_i = \\beta_0 + \\beta_1b_1(x_i)+\\beta_2b_2(x_i)+...+\\beta_{K+3}b_{K+3}(x_i)+\\epsilon_i$$ \n",
    "  \n",
    "for an appropriate choice of basis functions $b_1,b_2...b_{K+3}$  \n",
    "  \n",
    "Suppose that we have K knots $c_1, c_2...,c_K$, one simple way to represent a cubic spline is that we perform least squares regression with an intercept and $3+K$ predictors, of the form $X, X^2, X^3, h(X,c_1),h(X,c_2),...,h(X,c_K)$   \n",
    "  \n",
    "$$y_i = \\beta_0 + \\beta_1X+\\beta_2X^2+\\beta_3X^3+\\beta_4h(X,c_1)+...+\\beta_{K+3}h(x,c_K) +\\epsilon_i$$\n",
    "   \n",
    "### Inconvenients:\n",
    "Splines can have high variance at the outer range of the predictors (when X is very small or very large).       \n",
    "    \n",
    "A **natural cubic spline** is a regression spline with additional **boundary constraints**: the function is required to be linear at the boundary (in the region where $X$ is\n",
    "smaller than the smallest knot, or larger than the largest knot). This additional constraint means that natural splines generally produce more stable estimates at the boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Number and Locations of the Knots  <a name=\"numberknot\"></a>\n",
    "### Where should we put the knots? \n",
    "The regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. Hence, one option is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable. In practice, we cand place knots at uniform quantiles of the data.  \n",
    "### How many knots should we use or equivalently how many degrees of freedom should our spline contain? \n",
    "=> Use **Cross-validation**: we remove a portion of the data (say 10%), fit a spline with a certain number of knots to the remaining data, and then use the spline to make predictions for the held-out portion. We repeat this process multiple times until each observation has been left out once, and then compute the overall cross-validated RSS. This procedure can be repeated for different numbers of knotsK. Then the value of $K$ giving the smallest RSS is chosen.\n",
    "  \n",
    "### Comparison to Polynomial Regression? \n",
    "Regression splines often give superior results to polynomial regression. \n",
    "1. Because Polynomials must use a high degree to produce flexible fits, splines introduce flexibility by increasing the number of knots but keeping the degree fixed. So generally this approach produces more stable estimates.   \n",
    "2.  Splines also allow us to place more knots, and hence flexibility, over regions where the function $f$ seems to be changing rapidly, and fewer knots where $f$ appears more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing Splines <a name=\"smooth\"></a>\n",
    "In fitting a smooth curve to a set of data, what we really want to do is find some function, say g(x) that fits the observed data well:  \n",
    "We want $RSS = \\displaystyle\\sum_{i=1}^n(y_i - g(x_i))^2$ to be small.  \n",
    "To make g is smooth (don't overfit data), a natural approach is to find the function g that minimizes \n",
    "$$ \\displaystyle\\sum_{i=1}^n(y_i - g(x_i))^2 + \\lambda \\int g{''}(t)^2dt$$ \n",
    "  \n",
    "where $\\lambda$ is a nonnegative tuning parameter or bias-variance trade-off parameter.   \n",
    "   \n",
    "### 1. Interpretation of the term $\\int g{''}(t)^2dt$:   \n",
    "The term $\\lambda \\int g{''}(t)^2dt$ is a penalty term that penalizes the variability in g. The first derivative $g'(t)$  measures the slope of a function at $t$, and the second derivative corresponds to the amount by which the slope is changing. In other words, $\\int g''(t)^2dt$ is simply a measure of the total change in the function $g'(t)$ over its entire range.   \n",
    "If *g* is very smooth, $g'(t)$ will be close to constant and $\\int g''(t)^2dt$ will take on a small value.   \n",
    "If g is jumpy and variable then $g'(t)$ will vary significantly and $\\int g''(t)^2dt$ will take on a large value.  \n",
    "### 2. Interpretation of $\\lambda$\n",
    "When $\\lambda = 0$ then the penalty term has no effect, and so the function *g* will be very jumpy and will exactly interpolate the training observations.  \n",
    "When $\\lambda$ is very large, g will be perfectly smooth, it will be a straight line that passes as closely as possible to the training points.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Local Regression <a name = \"localregression\"> </a>\n",
    "This method involves computing the fit at a target point $x_0$ using only the nearby training observations. \n",
    "  \n",
    "### Algorithm: \n",
    "1. Gather the fraction $s = k/n$ of training points whose $x_i$ are closes to $x_0$   \n",
    "2. Assign a weight $K_{i0}= K(x_i, x_0)$ to each point in this neighborhood, so that the point furthest from $x_0$ has weight zero, and the closest has the highest weight. All but these k nearest neighbors get weight zero.   \n",
    "3. Fit a *weighted least squares regression* of the $y_i$ on the $x_i$ using the aforementioned weights, by finding $\\hat\\beta_0$ and $\\hat\\beta_1$ that minimize \n",
    "$$ \\displaystyle\\sum_{i=1}^n K_{i0}(y_i - \\beta_0 - \\beta_1x_i)^2 $$  \n",
    "  \n",
    "4. The fitted value at $x_0$ is given by $\\hat{f}(x_0) = \\hat\\beta_0 + \\hat\\beta_1 x_0$  \n",
    "    \n",
    "### Procedure \n",
    "To implement the procedure, need to specify:  \n",
    "1. Fraction of data in each local neighborhood (smoothing parameter $s$)   \n",
    "2. Weight functin for the least square fit\n",
    "3. Degree of locally fitted polynomial (linear, quadratic...)\n",
    "4. Number of iterative weighted least square fits.  \n",
    "   \n",
    "### Interpretation: \n",
    "#### 1. The span $s$ \n",
    "The smaller the value of $s$, the more local and wiggly will be our fit   \n",
    "A very large value of $s$ with lead to a global fit to the data using all of the training observations.  \n",
    "We can use cross-validation to choose $s$ or we can specify it directly   \n",
    "   \n",
    "Another interpretation of $s$ is $h(x_0)$  \n",
    "For computational and theoretical purposes we will define this weight function so that only values within a *smoothing window* $[x_0 + h(x_0), x_0-h(x_0)]$ will be considered in the estimate of $f(x_0)$ \n",
    "\n",
    "#### 2. The weight $K_{i0}$\n",
    "Purposes: the point furthest from $x_0$ has weight zero, and the closest has the highest weight. All but these k nearest neighbors get weight zero.   \n",
    "This is achieved by considering weight functions that are 0 outside of $[-1,1]$.   \n",
    "For example Tukey's tri-weight function: \n",
    "$$\n",
    " W(u) = \n",
    "  \\begin{cases} \n",
    "   (1- |u|^3)^3 & \\text{if } |u| \\leq 1 \\\\\n",
    "   0     & \\text{if } |u| >1 \n",
    "  \\end{cases}\n",
    "$$  \n",
    "So $$K(x_i,x_0) = W\\big(\\frac{x_i-x_0}{h(x)}\\big) = \\bigg( 1- \\big( \\frac{d(x_j,x_i)}{max d(x_l,x_i)} \\big)^3 \\bigg)^3$$ \n",
    "#### 3. Degree of locally fitted polynomial   \n",
    "It uses the **Taylor-decomposition** of the function f on each point, and a local weigthing of the points, to find the values.   \n",
    "$$f(x) = f(x_0) + \\displaystyle\\sum_{k=1}^K \\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k + o(|x-x_0|^K)   \\quad \\text{as } |x-x_0| \\to 0 $$  \n",
    "    \n",
    "    \n",
    "Case1 : If degree = 1 => linear regression  \n",
    "We tend to minimize $ \\displaystyle\\sum_{i=1}^n K_{i0}(y_i - \\beta_0 - \\beta_1x_i)^2 $   \n",
    "   \n",
    "Case2 : if degree = 2 => quadratic regression   \n",
    "$$f(x) \\approx \\beta_0 + \\beta_1(x-x_0) + \\frac{1}{2}\\beta_2(x-x_0)^2 \\quad \\text{for } x \\in [x_0-h(x_0),x_0+h(x_0)] $$\n",
    "    \n",
    "So, we tend to minimize $ \\displaystyle\\sum_{i=1}^n K_{i0}\\big(y_i - \\beta_0 - \\beta_1x_i - \\frac{1}{2}\\beta_2(x-x_0)\\big)^2 $  \n",
    "    \n",
    "#### Practice: In python, we can visualize this method:    \n",
    "``` python \n",
    "import pyqt_fit.nonparam_regression as smooth\n",
    "from pyqt_fit import npr_methods\n",
    "#linear (degree =1)\n",
    "k1 = smooth.NonParamRegression(xs, ys, method=npr_methods.LocalPolynomialKernel(q=1))\n",
    "# quadratic (degree = 2)\n",
    "k2 = smooth.NonParamRegression(xs, ys, method=npr_methods.LocalPolynomialKernel(q=2))\n",
    "#cubic (degree = 3)\n",
    "k3 = smooth.NonParamRegression(xs, ys, method=npr_methods.LocalPolynomialKernel(q=3))\n",
    "k1.fit(); k2.fit(); k3.fit()\n",
    "plt.figure()\n",
    "plt.plot(xs, ys, 'o', alpha=0.5, label='Data')\n",
    "plt.plot(grid, k3(grid), 'y', label='cubic', linewidth=2)\n",
    "plt.plot(grid, k2(grid), 'k', label='quadratic', linewidth=2)\n",
    "plt.plot(grid, k1(grid), 'g', label='linear', linewidth=2)\n",
    "plt.plot(grid, f(grid), 'r--', label='Target', linewidth=2)\n",
    "plt.legend(loc='best')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyqt_fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e61dc0d5d6ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mpyqt_fit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyqt_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonparam_regression\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msmooth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyqt_fit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnpr_methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyqt_fit'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyqt_fit import plot_fit\n",
    "import pyqt_fit.nonparam_regression as smooth\n",
    "from pyqt_fit import npr_methods\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 3*np.cos(x/2) + x**2/5 + 3\n",
    "xs = np.random.rand(200) * 10\n",
    "ys = f(xs) + 2*np.random.randn(*xs.shape)\n",
    "\n",
    "\n",
    "grid = np.r_[0:10:512j]\n",
    "plt.plot(grid, f(grid), 'r--', label='Reference')\n",
    "plt.plot(xs, ys, 'o', alpha=0.5, label='Data')\n",
    "plt.legend(loc='best')\n",
    "k1 = smooth.NonParamRegression(xs, ys, method=npr_methods.LocalPolynomialKernel(q=1))\n",
    "k2 = smooth.NonParamRegression(xs, ys, method=npr_methods.LocalPolynomialKernel(q=2))\n",
    "k3 = smooth.NonParamRegression(xs, ys, method=npr_methods.LocalPolynomialKernel(q=3))\n",
    "k12 = smooth.NonParamRegression(xs, ys, method=npr_methods.LocalPolynomialKernel(q=12))\n",
    "k1.fit(); k2.fit(); k3.fit(); k12.fit()\n",
    "plt.figure()\n",
    "plt.plot(xs, ys, 'o', alpha=0.5, label='Data')\n",
    "plt.plot(grid, k12(grid), 'b', label='polynom order 12', linewidth=2)\n",
    "plt.plot(grid, k3(grid), 'y', label='cubic', linewidth=2)\n",
    "plt.plot(grid, k2(grid), 'k', label='quadratic', linewidth=2)\n",
    "plt.plot(grid, k1(grid), 'g', label='linear', linewidth=2)\n",
    "plt.plot(grid, f(grid), 'r--', label='Target', linewidth=2)\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Additive Models \n",
    "Generalized Additive Models (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity.  \n",
    "The model become \n",
    "$$y_i = \\beta_0 + \\displaystyle\\sum_{j=1}^p f_j(x_{ij}) + \\epsilon_i$$ \n",
    "$$ = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2})+...+ f_p(x_{ip}) + \\epsilon_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
