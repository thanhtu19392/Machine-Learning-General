{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)  \n",
    "  \n",
    "This article will focus on explaining in detail both on theory and on coding (python) about **Singular Value Decomposition (SVD)** \n",
    "\n",
    "## 1. Some Math Definition\n",
    "### 1.1 Diagonal matrix  \n",
    "Definition: A diagonal matrix is a matrix in which the entries outside the main diagonal are all zero.\n",
    "\n",
    "### 1.2 Orthogonal matrix\n",
    "Definition: An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors), i.e. $$\\mathcal{Q}^T\\mathcal{Q}= \\mathcal{Q}\\mathcal{Q}^T = \\mathcal{I}$$ \n",
    "with $\\mathcal{I}$ is the identity matrix\n",
    "\n",
    "### 1.3 Properties \n",
    "1. $\\mathcal{Q}^{-1} = \\mathcal{Q}^T$\n",
    "\n",
    "2. If $\\mathcal{Q}$ is a orthogonal matrix, $\\mathcal{Q}^T$ is also a orthogonal matrix\n",
    "\n",
    "3. Determinant of an orthogonal matrix is equal 1 or -1. \n",
    "\n",
    "4. Suppose we have 2 vectors $x,y \\in \\mathcal{R}^m$ and an orthogonal matrix $\\mathcal{Q} \\in \\mathcal{R}^{m.m}$   \n",
    "We can use this matrix to rotate two vectors. They will become $\\mathcal{Q}x, \\mathcal{Q}y$ \n",
    "$$(\\mathcal{Q}x)^T (\\mathcal{Q}y) = x^T\\mathcal{Q}^T\\mathcal{Q}y = x^Ty$$\n",
    "\n",
    "So the rotation doesn't change the product of two vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Vector Terminology\n",
    "#### 1.3.1 Vector Length\n",
    "The length of a vector is found by squaring each component, adding them all together, and taking the square root of the sum.  \n",
    "If $\\vec{v}$ is a vector, its length is denoted by $|\\vec{v}|$. \n",
    "$$ |\\vec{v}| = \\sqrt{\\displaystyle\\sum_{i=1}^{n} v_i^2}$$\n",
    "#### 1.3.2 Scalar Multiplication\n",
    "Multiplying a scalar (real number) times a vector means multiplying every component by that real number to yield a new vector  \n",
    "\n",
    "#### 1.3.3 Inner Product\n",
    "The inner product of two vectors (also called the **dot product or scalar product**) deﬁnes multiplication of vectors. It is found by multiplying each component in $\\vec{v_1}$ by the component in $\\vec{v_1}$ in the same position and adding them all together to yield a scalar value. The inner product is only deﬁned for vectors of the same dimension. The inner product of two vectors is denoted ($\\vec{v_1}$,$\\vec{v_2}$) or $\\vec{v_1}$ ·$\\vec{v_2}$ (the dot product). \n",
    "$$(\\vec{v_1},\\vec{v_2})= \\vec{v_1} ·\\vec{v_2}= \\displaystyle\\sum_{i=1}^{n} x_iy_i$$\n",
    "#### 1.3.4 Orthogonality\n",
    "Two vectors are orthogonal to each other if their inner product equals zero. In twodimensional space this is equivalent to saying that the vectors are **perpendicular**, or that the only angle between them is a $90◦$ angle\n",
    "#### 1.3.5 Normal Vector (Unit Vector) \n",
    "A normal vector (or unit vector) is a vector of length 1.  \n",
    "*Any vector with an initial length > 0 can be normalized by dividing each component in it by the vector’s length*\n",
    "#### 1.3.6 Orthonormal Vectors\n",
    "Vectors of unit length that are **orthogonal** to each other are said to be **orthonormal**\n",
    "#### 1.3.7 Gram-Schmidt Orthonormalization Process\n",
    "The Gram-Schmidt orthonormalization process is a method for converting a set of vectors into a set of orthonormal vectors.  \n",
    "Find an [example in this document (page 7)](https://datajobs.com/data-science-repo/SVD-Tutorial-[Kirk-Baker].pdf)   \n",
    "\n",
    "\n",
    "\n",
    "#### 1.3.8 Eigenvectors and Eigenvalues\n",
    "An *eigenvector* is a nonzero vector that satisﬁes the equation $$A\\vec{v} = \\lambda\\vec{v}$$\n",
    "where A is a square matrix, λ is a scalar, and $\\vec{v}$ is the *eigenvector*. λ is called an *eigenvalue*  \n",
    "  \n",
    "You can ﬁnd eigenvalues and eigenvectors by treating a matrix as a system of linear equations and solving for the values of the variables that make up the components of the eigenvector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2. Problem \n",
    "A matrix $\\mathcal{A}$ dimension $n$ x $d$. Treat rows of matrix $\\mathcal{A}$ as n points in a d-dimensional space.  \n",
    "The problem is to find the best *k*-dimensional subspace with respect to the set of points. \n",
    "*Here 'best' means minimize the sum of the squares of the perpendicular distances of the points to the subspace.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Lets tackle this problem \n",
    "### 3.1 Begin with 1-dimensional subspace\n",
    "Problem: *best least squares fit*  \n",
    "Finding the best fitting line through the origin with respect\n",
    "to a set of points ${x_i|1 ≤ i ≤ n}$ in the plane means minimizing the sum of the squared\n",
    "distances of the points to the line.  \n",
    "Consider projecting a point $x_i$ onto a line through the origin. then: \n",
    "$$ x_{i1}^2 + x_{i1}^2 +..... + x_{i1}^2 = (length-of-projection)^2 + (distance-of-point-to- line)^2 $$\n",
    "To minimize the sum of the squares of the distances to the line, one could the sum of the squares of the lengths of the projections onto the line because the sum $x_{i1}^2 + x_{i1}^2 +..... + x_{i1}^2$ is constant\n",
    "\n",
    "\n",
    "### 3.2 Singular Vectors and singular values\n",
    "\n",
    "Consider the rows of A as n points in a *d*-dimensional space. Consider the best fit line through the origin. Let *v* be a unit vector along this line.  \n",
    "=> the length of the projection of $a_i$, the $i^{th}$ row of A onto $v$ is $|a_i.v|$\n",
    "=> the best fit line is the one maximizing the $|Av|^2$  \n",
    "   \n",
    "#### First singular vector   \n",
    "Problem: *find the best fit 1-dimensional subspace for a matrix A*  \n",
    "*The first singular vector* , $v_1$ of A, which is a column vector, as the best fit line through the origin for the n points in *d*-dimensional space that are rows of A.  \n",
    "$$v_1 = \\arg\\operatorname*{max}_{|v|=1}|Av|$$  \n",
    "${\\sigma}_1 = |Av_1|$ is called the **first singular value** of A  \n",
    "${\\sigma}_1^2$ is the sum of the squares of the projections of the points to the line determined by $v_1$  \n",
    "  \n",
    "#### Second singular vector     \n",
    "Problem: *find the best fit 2-dimensional subspace containing $v_1$ for a matrix A*   \n",
    "*The first singular vector* , $v_2$ of A, is defined by the best fit line perpendicular to $v_1$  \n",
    "$$v_2 = \\operatorname*{arg\\,max}_{v\\perp{v_1}, |v| =1} |Av| $$  \n",
    "${\\sigma}_2 = |Av_2|$ is called the **second singular value** of A \n",
    "  \n",
    "Why $v_2\\perp{v_1}$?  \n",
    "For every 2-dimensional subspace containing $v_1$, the sum of squared lengths of the projections onto the subspace equals the sum of squared projections onto $v_1$ plus the sum of squared projections along a vector perpendicular to $v_1$ in the subspace  \n",
    "  \n",
    "#### $r^{th}$ singular vector \n",
    "The greedy algorithm stops when we have found $v_1, v_2,...,v_r$ as singular vectors and \n",
    "$$ \\operatorname*{arg\\,max}_{v\\perp{v_1},{v_2}...{v_r}, |v| =1} |Av| = 0 $$  \n",
    "\n",
    "Remark: r is rank of matrix A\n",
    "    \n",
    "**Important note:**   \n",
    "We see that singular vectors $v_i$ are orthonormal because they are unit vectors and they are orthogonal to each other\n",
    "  \n",
    "### 3.3 Left singular vectors  \n",
    "$u_i$ are left singular vectors of A if $$u_i = \\frac{1}{{\\sigma}_i(A)} Av_i$$  \n",
    "   \n",
    "We can demonstrate that $u_i$ are orthogonal (see [this](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf))   \n",
    "\n",
    "### 3.4 Singular Vector Decomposition\n",
    "**Theorem**: Let a be an $n$ x $d$ matrix with right singular vectors $v_1, v_2, ... v_r$, left singular vectors $u_1, u_2,..., u_r$ and corresponding values $\\sigma_1, \\sigma_2...,\\sigma_r$ \n",
    "\n",
    "$$ A = \\displaystyle\\sum_{i=1}^{r} \\sigma_iu_iv_i^T$$  \n",
    "  \n",
    "$$ A = UDV^T $$  \n",
    "where the columns of U and V consist of the left and right singular vectors, respectively, and D is a diagonal matrix whose diagonal entries are the singular values of A.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example\n",
    "Now we will use python in order to calculate the left singular vectors, right singular vectors and corresponding singular values   \n",
    "   \n",
    "Coding in python :  \n",
    "```python \n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    " \n",
    "from random import normalvariate\n",
    "from math import sqrt\n",
    " \n",
    "def randomUnitVector(n):\n",
    "    unnormalized = [normalvariate(0, 1) for _ in range(n)]\n",
    "    theNorm = sqrt(sum(x * x for x in unnormalized))\n",
    "    return [x / theNorm for x in unnormalized]\n",
    " \n",
    "def svd_1d(A, epsilon=1e-10):\n",
    "    ''' The one-dimensional SVD '''\n",
    " \n",
    "    n, m = A.shape\n",
    "    x = randomUnitVector(m)\n",
    "    lastV = None\n",
    "    currentV = x\n",
    "    B = np.dot(A.T, A)\n",
    " \n",
    "    iterations = 0\n",
    "    while True:\n",
    "        iterations += 1\n",
    "        lastV = currentV\n",
    "        currentV = np.dot(B, lastV)\n",
    "        currentV = currentV / norm(currentV)\n",
    " \n",
    "        if abs(np.dot(currentV, lastV)) > 1 - epsilon:\n",
    "            print(\"converged in {} iterations!\".format(iterations))\n",
    "            return currentV\n",
    "\n",
    "def svd(A, epsilon=1e-10):\n",
    "    n, m = A.shape\n",
    "    svdSoFar = []\n",
    " \n",
    "    for i in range(m):\n",
    "        matrixFor1D = A.copy()\n",
    " \n",
    "        for singularValue, u, v in svdSoFar[:i]:\n",
    "            matrixFor1D -= singularValue * np.outer(u, v)\n",
    " \n",
    "        v = svd_1d(matrixFor1D, epsilon=epsilon)  # next right singular vector\n",
    "        u_unnormalized = np.dot(A, v)\n",
    "        sigma = norm(u_unnormalized)  # next singular value\n",
    "        u = u_unnormalized / sigma # next left singular vector\n",
    " \n",
    "        svdSoFar.append((sigma, u, v))\n",
    " \n",
    "    # transform it into matrices of the right shape\n",
    "    singularValues, us, vs = [np.array(x) for x in zip(*svdSoFar)]\n",
    " \n",
    "    return singularValues, us.T, vs\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    movieRatings = np.array([\n",
    "        [2, 5, 3],\n",
    "        [1, 2, 1],\n",
    "        [4, 1, 1],\n",
    "        [3, 5, 2],\n",
    "        [5, 3, 1],\n",
    "        [4, 5, 5],\n",
    "        [2, 4, 2],\n",
    "        [2, 2, 5],\n",
    "    ], dtype='float64')\n",
    " \n",
    "    print(svd_1d(movieRatings))\n",
    "\n",
    "theSVD = svd(movieRatings)\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Utilize the library \n",
    "```python \n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    " \n",
    "movieRatings = [\n",
    "    [2, 5, 3],\n",
    "    [1, 2, 1],\n",
    "    [4, 1, 1],\n",
    "    [3, 5, 2],\n",
    "    [5, 3, 1],\n",
    "    [4, 5, 5],\n",
    "    [2, 4, 2],\n",
    "    [2, 2, 5],\n",
    "]\n",
    " \n",
    "U, singularValues, V = svd(movieRatings)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applications \n",
    "\n",
    "### 5.1 Data compression\n",
    "### 5.2 Noise reduction\n",
    "### 5.3 Data analysis \n",
    "### 5.4 Recommendation problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Reference:  \n",
    "1. http://www.ams.org/samplings/feature-column/fcarc-svd\n",
    "2. [source pdf explaining math formula](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf)\n",
    "3. [Example](https://datajobs.com/data-science-repo/SVD-Tutorial-[Kirk-Baker].pdf)\n",
    "4. [Source Vietnamese](http://machinelearningcoban.com/2017/06/07/svd/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
