{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_decision_tree(data, attributes, target_attr, fitness_func):\n",
    "    \"\"\"\n",
    "    data: training data\n",
    "    Returns a new decision tree based on the examples given.\n",
    "    \"\"\"\n",
    "    data    = data[:] #a copy of data list being passed into the function\n",
    "    vals    = [record[target_attr] for record in data]\n",
    "    default = majority_value(data, target_attr)\n",
    "\n",
    "    # If the dataset is empty or the attributes list is empty, return the\n",
    "    # default value. When checking the attributes list for emptiness, we\n",
    "    # need to subtract 1 to account for the target attribute.\n",
    "    if not data or (len(attributes) - 1) <= 0:\n",
    "        return default\n",
    "    # If all the records in the dataset have the same classification,\n",
    "    # return that classification.\n",
    "    elif vals.count(vals[0]) == len(vals):\n",
    "        return vals[0]\n",
    "    else:\n",
    "        # Choose the next best attribute to best classify our data\n",
    "        best = choose_attribute(data, attributes, target_attr,\n",
    "                                fitness_func)\n",
    "\n",
    "        # Create a new decision tree/node with the best attribute and an empty\n",
    "        # dictionary object--we'll fill that up next.\n",
    "        tree = {best:{}}\n",
    "\n",
    "        # Create a new decision tree/sub-node for each of the values in the\n",
    "        # best attribute field\n",
    "        for val in get_values(data, best):\n",
    "            # Create a subtree for the current value under the \"best\" field\n",
    "            subtree = create_decision_tree(\n",
    "                get_examples(data, best, val),\n",
    "                [attr for attr in attributes if attr != best],\n",
    "                target_attr,\n",
    "                fitness_func)\n",
    "\n",
    "            # Add the new subtree to the empty dictionary object in our new\n",
    "            # tree/node we just created.\n",
    "            tree[best][val] = subtree\n",
    "\n",
    "    return tree\n",
    "\n",
    "def entropy(data, target_attr):\n",
    "    \"\"\"\n",
    "    Calculates the entropy of the given data set for the target attribute.\n",
    "    \"\"\"\n",
    "    val_freq     = {}\n",
    "    # Dict hold all the values found in the data set passed into this function (key)\n",
    "    # and the frequency at which each value appears in the data set (value)\n",
    "    data_entropy = 0.0\n",
    "\n",
    "    # Calculate the frequency of each of the values in the target attr\n",
    "    for record in data:\n",
    "        if (val_freq.has_key(record[target_attr])):\n",
    "            val_freq[record[target_attr]] += 1.0\n",
    "        else:\n",
    "            val_freq[record[target_attr]]  = 1.0\n",
    "\n",
    "    # Calculate the entropy of the data for the target attribute\n",
    "    for freq in val_freq.values():\n",
    "        data_entropy += (-freq/len(data)) * math.log(freq/len(data), 2) \n",
    "        \n",
    "    return data_entropy\n",
    "\n",
    "def gain(data, attr, target_attr):\n",
    "    \"\"\"\n",
    "    Calculates the information gain (reduction in entropy) that would\n",
    "    result by splitting the data on the chosen attribute (attr).\n",
    "    \"\"\"\n",
    "    val_freq       = {}\n",
    "    subset_entropy = 0.0\n",
    "\n",
    "    # Calculate the frequency of each of the values in the target attribute\n",
    "    for record in data:\n",
    "        if (val_freq.has_key(record[attr])):\n",
    "            val_freq[record[attr]] += 1.0\n",
    "        else:\n",
    "            val_freq[record[attr]]  = 1.0\n",
    "\n",
    "    # Calculate the sum of the entropy for each subset of records weighted\n",
    "    # by their probability of occuring in the training set.\n",
    "    for val in val_freq.keys():\n",
    "        val_prob        = val_freq[val] / sum(val_freq.values())\n",
    "        data_subset     = [record for record in data if record[attr] == val]\n",
    "        subset_entropy += val_prob * entropy(data_subset, target_attr)\n",
    "\n",
    "    # Subtract the entropy of the chosen attribute from the entropy of the\n",
    "    # whole data set with respect to the target attribute (and return it)\n",
    "    return (entropy(data, target_attr) - subset_entropy)\n",
    "\n",
    "def choose_attribute(data, attributes, target_attr, fitness_func):\n",
    "    best = attributes[0]\n",
    "    maxGain = 0\n",
    "    for attr in attributes: \n",
    "        newGain = gain(data, attr, target_attr)\n",
    "        if newGain > maxGain:\n",
    "            maxGain = newGain\n",
    "            best= attr\n",
    "    return best\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
