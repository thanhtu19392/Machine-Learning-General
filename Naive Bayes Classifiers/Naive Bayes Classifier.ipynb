{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The Naive Bayes Classfier technique is based on the so-called Bayesian theorem and is particularly suited when the dimensionality of the inputs is high. Despite its simplicity, Naive Bayes can often outperform more sophisticated classification methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic model \n",
    "\n",
    "given a problem instance to be classified, represented by a vector ${\\displaystyle \\mathbf {x} =(x_{1},\\dots ,x_{n})}$ representing some n features (independent variables), it assigns to this instance probabilities    \n",
    "$${\\displaystyle p(C_{k}\\mid x_{1},\\dots ,x_{n})\\,}$$\n",
    "for each of k possible outcomes or classes ${\\displaystyle C_{k}}$  \n",
    "  \n",
    "The problem with the above formulation is that if the number of features n is large or if a feature can take on a large number of values, then basing such a model on probability tables is infeasible.   \n",
    "    \n",
    "Using Bayes' theorem, the conditional probability can be decomposed as\n",
    "\n",
    "$$ {\\displaystyle p(C_{k}\\mid \\mathbf {x} )={\\frac {p(C_{k})\\ p(\\mathbf {x} \\mid C_{k})}{p(\\mathbf {x} )}}\\,}$$\n",
    "   \n",
    "$${\\displaystyle {\\mbox{posterior}}={\\frac {{\\mbox{prior}}\\times {\\mbox{likelihood}}}{\\mbox{evidence}}}\\,}$$  \n",
    "  \n",
    "In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on ${\\displaystyle C}$ and the values of the features ${\\displaystyle F_{i}}$ are given, so that the denominator is effectively constant. The numerator is equivalent to the joint probability model\n",
    "\n",
    "${\\displaystyle p(C_{k},x_{1},\\dots ,x_{n})\\,}$   \n",
    "  \n",
    "$${\\displaystyle \n",
    "{\\begin{aligned}\n",
    "p(C_{k},x_{1},\\dots ,x_{n})&=p(x_{1},\\dots ,x_{n},C_{k})\\\\\n",
    "&=p(x_{1}\\mid x_{2},\\dots ,x_{n},C_{k})p(x_{2},\\dots ,x_{n},C_{k})\\\\\n",
    "&=p(x_{1}\\mid x_{2},\\dots ,x_{n},C_{k})p(x_{2}\\mid x_{3},\\dots ,x_{n},C_{k})p(x_{3},\\dots ,x_{n},C_{k})\\\\&=\\dots \\\\&=p(x_{1}\\mid x_{2},\\dots ,x_{n},C_{k})p(x_{2}\\mid x_{3},\\dots ,x_{n},C_{k})\\dots p(x_{n-1}\\mid x_{n},C_{k})p(x_{n}\\mid C_{k})p(C_{k})\\\\\n",
    "\\end{aligned}}}$$   \n",
    "   \n",
    "**The \"naive\" conditional independence assumptions**    \n",
    "Assume that each feature ${\\displaystyle F_{i}}$ is conditionally independent of every other feature ${\\displaystyle F_{j}}$ for ${\\displaystyle j\\neq i}$, given the category ${\\displaystyle C}$    \n",
    "${\\displaystyle p(x_{i}\\mid x_{i+1},\\dots ,x_{n},C_{k})=p(x_{i}\\mid C_{k})\\,}$    \n",
    "   \n",
    "The joint model can be expressed as     \n",
    "\n",
    "${\\displaystyle {\\begin{aligned}p(C_{k}\\mid x_{1},\\dots ,x_{n})&\\varpropto p(C_{k},x_{1},\\dots ,x_{n})\\\\&\\varpropto p(C_{k})\\ p(x_{1}\\mid C_{k})\\ p(x_{2}\\mid C_{k})\\ p(x_{3}\\mid C_{k})\\ \\cdots \\\\&\\varpropto p(C_{k})\\prod _{i=1}^{n}p(x_{i}\\mid C_{k})\\,.\\end{aligned}}}$    \n",
    "This means that under the above independence assumptions, the conditional distribution over the class variable ${\\displaystyle C}$ is:     \n",
    "\n",
    "${\\displaystyle p(C_{k}\\mid x_{1},\\dots ,x_{n})={\\frac {1}{Z}}p(C_{k})\\prod _{i=1}^{n}p(x_{i}\\mid C_{k})} $     \n",
    "where the evidence ${\\displaystyle Z=p(\\mathbf {x} )}$ is a scaling factor dependent only on ${\\displaystyle x_{1},\\dots ,x_{n}}$, that is, a constant if the values of the feature variables are known.    \n",
    "    \n",
    "## Constructing a classifier from the probability model\n",
    "The corresponding classifier, a Bayes classifier, is the function that assigns a class label ${\\displaystyle {\\hat {y}}=C_{k}}$  for some k as follows (maximum a posteriori or MAP decision rule):     \n",
    "\n",
    "${\\displaystyle {\\hat {y}}={\\underset {k\\in \\{1,\\dots ,K\\}}{\\operatorname {argmax} }}\\ p(C_{k})\\displaystyle \\prod _{i=1}^{n}p(x_{i}\\mid C_{k}).} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "### Fruit Example \n",
    "Let's say that we have data on 1000 pieces of fruit. They happen to be Banana, Orange or some Other Fruit. We know 3 characteristics about each fruit:   \n",
    "\n",
    "1. Whether it is Long\n",
    "2. Whether it is Sweet\n",
    "3. If its color is Yellow.   \n",
    "   \n",
    "|Type        |  Long | Not Long  | Sweet | Not Sweet  | Yellow |Not Yellow|Total  |\n",
    "|----------- |-------| ----------| ------|------------|--------|----------|-------|\n",
    "|Banana      |  400  |    100    | 350   |    150     |  450   |  50      |  500  |\n",
    "|Orange      |    0  |    300    | 150   |    150     |  300   |   0      |  300  |\n",
    "|Other Fruit |  100  |    100    | 150   |     50     |   50   | 150      |  200  |          \n",
    "|Total       |  500  |    500    | 650   |    350     |  800   | 200      | 1000  |   \n",
    "    \n",
    "**\"Prior\" probabilities:**   \n",
    "P(Banana)      = 0.5 (500/1000)   \n",
    "P(Orange)      = 0.3    \n",
    "P(Other Fruit) = 0.2    \n",
    "    \n",
    "**Probability of \"Evidence\":**     \n",
    "p(Long)   = 0.5     \n",
    "P(Sweet)  = 0.65    \n",
    "P(Yellow) = 0.8     \n",
    "       \n",
    "**Likelihood Function:**       \n",
    "P(Long|Banana) = 0.8      \n",
    "P(Long|Orange) = 0    \n",
    " ....\n",
    "\n",
    "P(Yellow|Other Fruit)     =  50/200 = 0.25    \n",
    "P(Not Yellow|Other Fruit) = 0.75       \n",
    "   \n",
    "**Given a Fruit, how to classify it? **\n",
    "$P(\\text{Banana | Long, Sweet and Yellow}) \\\\   \n",
    "= \\frac{P(Long|Banana) * P(Sweet|Banana) * P(Yellow|Banana) * P(banana)}{P(Long) * P(Sweet) * P(Yellow)}\\\\\n",
    "= \\frac{0.8 * 0.7 * 0.9 * 0.5} {P(evidence)}\\\\\n",
    "= \\frac{0.252}{ P(evidence)}$\n",
    "\n",
    "\n",
    "$P(\\text{Orange | Long, Sweet and Yellow}) = 0$\n",
    "\n",
    "\n",
    "$P(\\text{Other Fruit | Long, Sweet and Yellow}) \\\\\n",
    " = \\frac{P(Long|Other fruit) * P(Sweet|Other fruit) * P(Yellow|Other fruit) * P(Other Fruit)}{P(evidence)}\\\\\n",
    "= \\frac{(100/200 * 150/200 * 50/200 * 200/1000) }{P(evidence)}\\\\\n",
    "= \\frac{0.01875}{P(evidence)}$    \n",
    "    \n",
    "So, we classify this Sweet/Long/Yellow fruit as likely to be a Banana.    \n",
    "\n",
    "### Pros and Cons of Naive Bayes   \n",
    "#### Pros:    \n",
    "\n",
    "1. It is easy and fast to predict class of test data set. It also perform well in multi class prediction     \n",
    "2. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.     \n",
    "3. It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).     \n",
    "\n",
    "#### Cons:      \n",
    "\n",
    "1. If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.     \n",
    "2. On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.      \n",
    "3. Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References:\n",
    "1. http://sebastianraschka.com/Articles/2014_naive_bayes_1.html  \n",
    "2. http://chrisstrelioff.ws/sandbox/2014/10/24/inferring_probabilities_a_second_example_of_bayesian_calculations.html#bayes-second-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
